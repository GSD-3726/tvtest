import asyncio
import http.cookies
import json
import re
import subprocess
import os
import requests
from time import time
from urllib.parse import quote, urljoin, urlparse

import m3u8
from aiohttp import ClientSession, TCPConnector
from multidict import CIMultiDictProxy

# ==============================================
# ã€æ ¸å¿ƒé…ç½®åŒºã€‘å¯ç›´æ¥ä¿®æ”¹ï¼Œæ— éœ€æ”¹ä¸‹æ–¹ä»£ç 
# ==============================================
# è¿œç¨‹é“¾æ¥åœ°å€ï¼ˆgh-proxyåŠ é€Ÿçš„rawåœ°å€ï¼Œç¡®ä¿è·å–çº¯æ–‡æœ¬ï¼‰
REMOTE_URL = "https://gh-proxy.com/https://raw.githubusercontent.com/GSD-3726/IPTV/master/output/result.txt"
OUTPUT_DIR = "output"
TXT_FILENAME = "result.txt"
M3U_FILENAME = "iptv.m3u"
REQUEST_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36"
}
URL_PATTERN = re.compile(r'https?://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]')

# æµ‹é€Ÿé…ç½®
SPEED_TEST_TIMEOUT = 10  # å•é“¾æ¥æµ‹é€Ÿè¶…æ—¶ï¼ˆç§’ï¼‰
SPEED_TEST_FILTER_HOST = False  # æŒ‰åŸŸåç¼“å­˜æµ‹é€Ÿç»“æœ
OPEN_FILTER_RESOLUTION = True  # å¼€å¯åˆ†è¾¨ç‡è¿‡æ»¤
MIN_RESOLUTION = 720  # æœ€ä½åˆ†è¾¨ç‡ï¼ˆå®½ï¼‰
MAX_RESOLUTION = 2160  # æœ€é«˜åˆ†è¾¨ç‡ï¼ˆå®½ï¼‰
OPEN_FILTER_SPEED = True  # å¼€å¯é€Ÿåº¦è¿‡æ»¤
MIN_SPEED = 1  # æœ€ä½æœ‰æ•ˆé€Ÿåº¦ï¼ˆMB/sï¼‰

# å›ºå®šé…ç½®
M3U8_HEADERS = ['application/x-mpegurl', 'application/vnd.apple.mpegurl', 'audio/mpegurl', 'audio/x-mpegurl']
DEFAULT_IPV6_DELAY = 0.1
DEFAULT_IPV6_RES = "1920x1080"
DEFAULT_IPV6_RESULT = {'speed': float("inf"), 'delay': DEFAULT_IPV6_DELAY, 'resolution': DEFAULT_IPV6_RES}
http.cookies._is_legal_key = lambda _: True
CACHE = {}  # æµ‹é€Ÿå…¨å±€ç¼“å­˜

# ==============================================
# ã€å·¥å…·å‡½æ•°åŒºã€‘æ‹‰å–é“¾æ¥/ç”Ÿæˆæ–‡ä»¶/åˆå§‹åŒ–
# ==============================================
def init_output_dir():
    """åˆå§‹åŒ–è¾“å‡ºç›®å½•"""
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
    print(f"âœ… è¾“å‡ºç›®å½•åˆå§‹åŒ–å®Œæˆï¼š{OUTPUT_DIR}")

def parse_result_file(url: str) -> list[dict]:
    """è§£æè¿œç¨‹æ–‡æœ¬æ–‡ä»¶ï¼Œè¿”å›åŒ…å«{'name', 'url'}çš„å­—å…¸åˆ—è¡¨"""
    items = []
    try:
        print(f"ğŸ” æ­£åœ¨æ‹‰å–è¿œç¨‹æ–‡ä»¶ï¼š{url}")
        resp = requests.get(url, headers=REQUEST_HEADERS, timeout=30)
        resp.raise_for_status()
        file_content = resp.text

        # è§£ææ–‡ä»¶å†…å®¹
        for line in file_content.splitlines():
            line = line.strip()
            if not line or '#genre#' in line:
                continue
            if ',' not in line:
                continue
            name, url = line.split(',', 1)
            items.append({'name': name.strip(), 'url': url.strip()})

        if not items:
            raise ValueError("æœªåŒ¹é…åˆ°ä»»ä½•æœ‰æ•ˆé“¾æ¥")
        print(f"âœ… æˆåŠŸè§£ææ–‡ä»¶ï¼Œæ‰¾åˆ° {len(items)} ä¸ªæœ‰æ•ˆé“¾æ¥")
    except Exception as e:
        print(f"âŒ è§£ææ–‡ä»¶å¤±è´¥ï¼š{e}")
        raise SystemExit(1)

    return items

def save_txt(items: list[dict]):
    """ä¿å­˜é“¾æ¥åˆ° TXT æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªé“¾æ¥ï¼‰"""
    txt_path = os.path.join(OUTPUT_DIR, TXT_FILENAME)
    with open(txt_path, 'w', encoding='utf-8') as f:
        for item in items:
            f.write(f"{item['name']},{item['url']}\n")
    print(f"âœ… TXTæ–‡ä»¶ç”Ÿæˆï¼š{txt_path}ï¼ˆ{len(items)}ä¸ªé“¾æ¥ï¼‰")

def save_m3u(items: list[dict]):
    """ç”Ÿæˆæ ‡å‡†IPTV M3Uæ–‡ä»¶ï¼ˆé€‚é…VLC/TVBox/PotPlayerï¼Œå«EPGï¼‰"""
    m3u_path = os.path.join(OUTPUT_DIR, M3U_FILENAME)
    with open(m3u_path, 'w', encoding='utf-8') as f:
        f.write("#EXTM3U x-tvg-url=\"https://epg.112114.xyz/epg.xml.gz\"\n\n")
        for idx, item in enumerate(items, 1):
            f.write(f"#EXTINF:-1,{item['name']}\n{item['url']}\n\n")
    print(f"âœ… M3Uæ–‡ä»¶ç”Ÿæˆï¼š{m3u_path}ï¼ˆ{len(items)}ä¸ªé¢‘é“ï¼‰")

# ==============================================
# ã€æµ‹é€Ÿæ ¸å¿ƒåŒºã€‘ä¿ç•™æ‰€æœ‰åŸæµ‹é€Ÿä¼˜åŒ–é€»è¾‘
# ==============================================
def print_startup_info():
    """æ‰“å°å¯åŠ¨ä¿¡æ¯å’Œé…ç½®"""
    print("=" * 60)
    print("ğŸ¬ IPTVé“¾æ¥æ‹‰å–+æµ‹é€Ÿå·¥å…·ï¼ˆå•æ–‡ä»¶ç‰ˆï¼‰")
    print("=" * 60)
    print(f"ğŸ”§ è¿è¡Œé…ç½®ï¼š")
    print(f"   - è¿œç¨‹é“¾æ¥ï¼š{REMOTE_URL}")
    print(f"   - æµ‹é€Ÿè¶…æ—¶ï¼š{SPEED_TEST_TIMEOUT}ç§’ | æœ€ä½é€Ÿåº¦ï¼š{MIN_SPEED}MB/s")
    print(f"   - åˆ†è¾¨ç‡è¿‡æ»¤ï¼š{MIN_RESOLUTION}x~{MAX_RESOLUTION}x | åŸŸåç¼“å­˜ï¼š{'å¼€å¯' if SPEED_TEST_FILTER_HOST else 'å…³é—­'}")
    print("=" * 60 + "\n")

# ==============================================
# ã€æ–°å¢ç¼ºå¤±å‡½æ•°ã€‘ä¿®å¤æœªå®šä¹‰é—®é¢˜ - æ ¸å¿ƒä¿®å¤ç‚¹1
# ==============================================
async def get_headers(url: str, headers: dict = None) -> CIMultiDictProxy:
    """è·å–é“¾æ¥å“åº”å¤´ï¼ˆå¼‚æ­¥ï¼‰ï¼Œç”¨äºåˆ¤æ–­å†…å®¹ç±»å‹ã€é‡å®šå‘"""
    if headers is None:
        headers = REQUEST_HEADERS.copy()
    async with ClientSession(connector=TCPConnector(ssl=False), trust_env=True) as session:
        try:
            async with session.head(url, headers=headers, timeout=SPEED_TEST_TIMEOUT, allow_redirects=True) as resp:
                return resp.headers
        except:
            # å¤´è¯·æ±‚å¤±è´¥åˆ™ç”¨getè¯·æ±‚è·å–å¤´
            async with session.get(url, headers=headers, timeout=SPEED_TEST_TIMEOUT, allow_redirects=True) as resp:
                return resp.headers

async def get_url_content(url: str, headers: dict = None) -> str:
    """è·å–é“¾æ¥æ–‡æœ¬å†…å®¹ï¼ˆå¼‚æ­¥ï¼‰ï¼Œç”¨äºè§£æm3u8"""
    if headers is None:
        headers = REQUEST_HEADERS.copy()
    try:
        async with ClientSession(connector=TCPConnector(ssl=False), trust_env=True) as session:
            async with session.get(url, headers=headers, timeout=SPEED_TEST_TIMEOUT) as resp:
                if resp.status == 200:
                    return await resp.text()
        return ""
    except:
        return ""

async def get_speed(data: dict) -> dict:
    """å•é“¾æ¥æµ‹é€Ÿå…¥å£ï¼ˆä¿®å¤æœªå®šä¹‰çš„æ ¸å¿ƒå‡½æ•°ï¼‰ï¼Œå°è£…ç¼“å­˜+æµ‹é€Ÿé€»è¾‘"""
    global CACHE
    name = data['name']
    url = data['url']
    host = data['host']
    headers = REQUEST_HEADERS.copy()

    # åŸŸåç¼“å­˜é€»è¾‘ï¼šå¼€å¯åˆ™ä¼˜å…ˆä½¿ç”¨ç¼“å­˜ç»“æœ
    if SPEED_TEST_FILTER_HOST and host in CACHE:
        result = CACHE[host].copy()
        result.update({'name': name, 'url': url, 'host': host})
        return result

    # æœªå‘½ä¸­ç¼“å­˜åˆ™æ‰§è¡Œå®é™…æµ‹é€Ÿ
    result = await get_result(url, headers)
    result.update({'name': name, 'url': url, 'host': host})

    # ç¼“å­˜ç»“æœï¼ˆå¼€å¯åŸŸåç¼“å­˜æ—¶ï¼‰
    if SPEED_TEST_FILTER_HOST and result['speed'] >= MIN_SPEED:
        CACHE[host] = {'speed': result['speed'], 'delay': result['delay'], 'resolution': result['resolution']}

    return result

# ==============================================
# ã€æµ‹é€Ÿæ ¸å¿ƒåŒºã€‘ä¿®å¤get_resultå†…çš„å˜é‡æœªå®šä¹‰é—®é¢˜ - æ ¸å¿ƒä¿®å¤ç‚¹2
# ==============================================
async def get_speed_with_download(url: str, headers: dict = None, session: ClientSession = None) -> dict:
    """ä¸‹è½½æµ‹é€Ÿï¼šè·å–å»¶è¿Ÿã€ä¸‹è½½å¤§å°ã€é€Ÿåº¦"""
    start_time = time()
    delay, total_size = -1, 0
    created_session = False
    if session is None:
        session = ClientSession(connector=TCPConnector(ssl=False), trust_env=True)
        created_session = True
    try:
        async with session.get(url, headers=headers, timeout=SPEED_TEST_TIMEOUT) as resp:
            if resp.status == 200:
                delay = int(round((time() - start_time) * 1000))
                async for chunk in resp.content.iter_any():
                    if chunk:
                        total_size += len(chunk)
    except:
        pass
    finally:
        total_time = max(time() - start_time, 0.001)  # é¿å…é™¤0
        speed = total_size / total_time / 1024 / 1024
        if created_session:
            await session.close()
        return {'speed': speed, 'delay': delay, 'size': total_size, 'time': total_time}

async def get_result(url: str, headers: dict = None) -> dict:
    """å•é“¾æ¥æµ‹é€Ÿï¼šä¸‹è½½æµ‹é€Ÿ + åˆ†è¾¨ç‡ + m3u8è§£æï¼ˆä¿®å¤start_timeæœªå®šä¹‰ã€è¡¥å……åˆ†è¾¨ç‡è§£æï¼‰"""
    info = {'speed': 0, 'delay': -1, 'resolution': DEFAULT_IPV6_RES}  # åˆå§‹åŒ–åˆ†è¾¨ç‡ï¼Œé¿å…None
    start_time = time()  # ä¿®å¤æœªå®šä¹‰çš„start_timeå˜é‡
    try:
        url = quote(url, safe=':/?$&=@[]%').partition('$')[0]
        res_headers = await get_headers(url, headers)
        # å¤„ç†é‡å®šå‘
        if location := res_headers.get('Location'):
            return await get_result(location, headers)
        # è§£æm3u8æµ
        content = await get_url_content(url, headers)
        if content and any(h in res_headers.get('Content-Type', '').lower() for h in M3U8_HEADERS):
            m3u8_obj = m3u8.loads(content)
            segment_urls = []
            # å¤„ç†å¤šç ç‡m3u8ï¼Œé€‰æœ€é«˜ç ç‡
            if m3u8_obj.playlists:
                best_playlist = max(m3u8_obj.playlists, key=lambda p: p.stream_info.bandwidth)
                playlist_content = await get_url_content(urljoin(url, best_playlist.uri), headers)
                if playlist_content:
                    sub_m3u8 = m3u8.loads(playlist_content)
                    segment_urls = [urljoin(url, s.uri) for s in sub_m3u8.segments]
                    # è§£æåˆ†è¾¨ç‡ï¼ˆä»m3u8æµä¿¡æ¯ä¸­æå–ï¼‰
                    if best_playlist.stream_info.resolution:
                        info['resolution'] = f"{best_playlist.stream_info.resolution[0]}x{best_playlist.stream_info.resolution[1]}"
            else:
                segment_urls = [urljoin(url, s.uri) for s in m3u8_obj.segments]
                # è§£æåˆ†è¾¨ç‡ï¼ˆå•ç ç‡m3u8ï¼‰
                if m3u8_obj.stream_info and m3u8_obj.stream_info.resolution:
                    info['resolution'] = f"{m3u8_obj.stream_info.resolution[0]}x{m3u8_obj.stream_info.resolution[1]}"
            # æµ‹é€Ÿm3u8ç‰‡æ®µï¼ˆè·³è¿‡ç¬¬ä¸€ä¸ªåˆå§‹åŒ–ç‰‡æ®µï¼Œå–åç»­5ä¸ªï¼‰
            if segment_urls:
                sample_segs = segment_urls[1:6] if len(segment_urls) > 1 else segment_urls
                tasks = [get_speed_with_download(seg, headers) for seg in sample_segs]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                # è¿‡æ»¤æœ‰æ•ˆç»“æœï¼ŒæŒ‰å¤§å°åŠ æƒè®¡ç®—
                valid_res = [r for r in results if isinstance(r, dict) and r['time'] > 0 and r['size'] > 0]
                if valid_res:
                    total_size = sum(r['size'] for r in valid_res)
                    weighted_time = sum((r['size']/total_size)*r['time'] for r in valid_res)
                    info['speed'] = total_size / weighted_time / 1024 / 1024
                    # è®¡ç®—å¹³å‡å»¶è¿Ÿï¼ˆè¿‡æ»¤æ— æ•ˆå»¶è¿Ÿï¼‰
                    valid_delays = [r['delay'] for r in valid_res if r['delay'] > 0]
                    info['delay'] = int(round(sum(valid_delays)/len(valid_delays))) if valid_delays else int(round((time()-start_time)*1000))
                else:
                    info['delay'] = int(round((time()-start_time)*1000))
        else:
            # ém3u8ç›´æ¥æµ‹é€Ÿ
            download_res = await get_speed_with_download(url, headers)
            info.update({'speed': download_res['speed'], 'delay': download_res['delay']})
    except Exception as e:
        pass
    return info

def get_sort_result(results: list[dict]) -> list[dict]:
    """è¿‡æ»¤å¹¶æ’åºæµ‹é€Ÿç»“æœï¼šæŒ‰é€Ÿåº¦ä»å¿«åˆ°æ…¢ï¼Œè¿‡æ»¤æ— æ•ˆé“¾æ¥"""
    valid_results = []
    for res in results:
        speed = res.get('speed') or 0
        delay = res.get('delay')
        reso = res.get('resolution')
        # è¿‡æ»¤å»¶è¿Ÿæ— æ•ˆçš„é“¾æ¥
        if delay == -1:
            continue
        # è¿‡æ»¤é€Ÿåº¦ä¸è¾¾æ ‡
        if OPEN_FILTER_SPEED and speed < MIN_SPEED:
            continue
        # è¿‡æ»¤åˆ†è¾¨ç‡ä¸è¾¾æ ‡
        if OPEN_FILTER_RESOLUTION and reso and reso != "éŸ³é¢‘æµ":
            try:
                res_w = int(reso.split('x')[0])
                if res_w < MIN_RESOLUTION or res_w > MAX_RESOLUTION:
                    continue
            except:
                continue
        valid_results.append(res)
    # æŒ‰é€Ÿåº¦é™åºæ’åºï¼Œé€Ÿåº¦ç›¸åŒåˆ™æŒ‰å»¶è¿Ÿå‡åº
    return sorted(valid_results, key=lambda x: (-(x.get('speed') or 0), x.get('delay') or 9999))

async def batch_speed_test(items: list[dict]) -> list[dict]:
    """æ‰¹é‡æµ‹é€Ÿå¹¶è¿”å›æœ‰æ•ˆé“¾æ¥ï¼ˆä¿®å¤è¿”å›å€¼ç±»å‹ï¼ŒåŒ¹é…ä¿å­˜å‡½æ•°å‚æ•°ï¼‰"""
    global CACHE
    CACHE = {}  # æ¸…ç©ºç¼“å­˜
    # æ„é€ æµ‹é€Ÿä»»åŠ¡ï¼šè¡¥å……name/host/ipv_typeï¼Œå…¼å®¹get_speedå…¥å‚
    test_tasks = []
    for item in items:
        try:
            host = urlparse(item['url']).netloc  # æ”¹ç”¨urlparseè§£æhostï¼Œæ›´å¥å£®
            test_tasks.append({
                'name': item['name'],
                'url': item['url'],
                'host': host,
                'ipv_type': 'ipv4'
            })
        except:
            continue
    # å¼‚æ­¥æ‰¹é‡æµ‹é€Ÿ
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡æµ‹é€Ÿï¼ˆå…±{len(test_tasks)}ä¸ªæœ‰æ•ˆä»»åŠ¡ï¼‰")
    tasks = [get_speed(data) for data in test_tasks]
    test_results = await asyncio.gather(*tasks, return_exceptions=False)
    # è¿‡æ»¤æ’åº
    sorted_res = get_sort_result(test_results)
    print(f"âœ… æµ‹é€Ÿå®Œæˆï¼Œä¿ç•™ {len(sorted_res)} ä¸ªæœ‰æ•ˆé“¾æ¥\n")
    return sorted_res  # è¿”å›å®Œæ•´å­—å…¸åˆ—è¡¨ï¼Œè€Œéä»…urlåˆ—è¡¨

# ==============================================
# ã€ä¸»å‡½æ•°ã€‘ä¿®å¤å‚æ•°ä¼ é€’é—®é¢˜ - æ ¸å¿ƒä¿®å¤ç‚¹3
# ==============================================
async def main():
    """ä¸»æ‰§è¡Œæµç¨‹"""
    # æ‰“å°å¯åŠ¨ä¿¡æ¯
    print_startup_info()
    # 1. åˆå§‹åŒ–ç›®å½•
    init_output_dir()
    # 2. æ‹‰å–è¿œç¨‹æ–‡ä»¶é“¾æ¥
    items = parse_result_file(REMOTE_URL)
    # 3. æ‰¹é‡æµ‹é€Ÿï¼ˆè¿”å›åŒ…å«name/urlçš„æœ‰æ•ˆå­—å…¸åˆ—è¡¨ï¼‰
    valid_items = await batch_speed_test(items)
    # 4. ç”Ÿæˆæ–‡ä»¶ï¼ˆä¿®å¤å‚æ•°ï¼šä¼ å…¥å®Œæ•´å­—å…¸åˆ—è¡¨ï¼Œè€Œéurlåˆ—è¡¨ï¼‰
    if valid_items:
        save_txt(valid_items)
        save_m3u(valid_items)
    else:
        print("âŒ æ— æœ‰æ•ˆé“¾æ¥ï¼Œæœªç”Ÿæˆæ–‡ä»¶")
    # æ‰§è¡Œå®Œæˆ
    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼è¾“å‡ºæ–‡ä»¶åœ¨ï¼šoutput ç›®å½•")
    print("=" * 60)

if __name__ == "__main__":
    # é€‚é…Windowsç³»ç»Ÿasyncioäº‹ä»¶å¾ªç¯é—®é¢˜
    if os.name == 'nt':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())
