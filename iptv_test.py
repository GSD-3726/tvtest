import aiohttp
import asyncio
import time
import statistics
from urllib.parse import urljoin
from datetime import datetime
import requests

# ===================== é…ç½®ï¼ˆä¿ç•™1080Pé˜ˆå€¼+æžé€Ÿå¹¶å‘ï¼‰=====================
RAW_TXT_URL = "https://gh-proxy.com/https://raw.githubusercontent.com/GSD-3726/IPTV/master/output/result.txt"
OUTPUT_FILE = "result.txt"
CONCURRENT_LIMIT = 8  # å¹¶å‘æ•°ï¼ˆ5-10æœ€ä½³ï¼Œé¿å…è¢«é™ï¼‰
TEST_SHARD_COUNT = 2  # m3u8åˆ†ç‰‡æµ‹è¯•æ•°
TIMEOUT_LIGHT = 2     # è½»é‡æ£€æµ‹è¶…æ—¶ï¼ˆç§’ï¼‰
TIMEOUT_DEEP = 3      # æ·±åº¦æµ‹é€Ÿè¶…æ—¶ï¼ˆç§’ï¼‰
# 1080Pæµç•…é˜ˆå€¼
FAIL_RATE_THRESHOLD = 0.05
AVG_TIME_THRESHOLD = 1.5
MAX_TIME_THRESHOLD = 4.0
MIN_HD_SHARD_SIZE = 102400  # 1080Påˆ†ç‰‡â‰¥100KB
SUPPORTED_PROTOCOLS = ("http://", "https://")

# ===================== å¼‚æ­¥å·¥å…·å‡½æ•°ï¼ˆä¿®å¤æ£€æµ‹é€»è¾‘ï¼Œç§»é™¤HEADï¼‰=====================
async def async_light_check(session, url):
    """æ›¿ä»£HEADçš„è½»é‡GETæ£€æµ‹ï¼šä»…ä¸‹è½½1KBæ•°æ®ï¼Œå…¼å®¹æ‰€æœ‰IPTVæº"""
    try:
        async with session.get(url, timeout=TIMEOUT_LIGHT) as resp:
            # ä»…è¯»å–1KBæ•°æ®ï¼Œä¸ä¸‹è½½å®Œæ•´å†…å®¹
            await resp.content.read(1024)
            return resp.status == 200
    except Exception:
        return False

async def async_download_hd(session, url, max_bytes):
    """å¼‚æ­¥ä¸‹è½½æŒ‡å®šå¤§å°æ•°æ®ï¼Œè¿”å›žï¼ˆè€—æ—¶ï¼Œæ˜¯å¦æˆåŠŸï¼Œä¸‹è½½å­—èŠ‚æ•°ï¼‰"""
    try:
        start_time = time.time()
        async with session.get(url, timeout=TIMEOUT_DEEP) as resp:
            if resp.status != 200:
                return 0, False, 0
            total_bytes = 0
            async for chunk in resp.content.iter_chunked(1024):
                total_bytes += len(chunk)
                if total_bytes >= max_bytes:
                    break
            cost_time = round(time.time() - start_time, 3)
            return cost_time, total_bytes >= 2*1024, total_bytes  # è‡³å°‘2KBè§†ä¸ºæœ‰æ•ˆ
    except Exception:
        return 0, False, 0

async def parse_m3u8_async(session, m3u8_url):
    """å¼‚æ­¥è§£æžm3u8ï¼Œä»…å–å‰Nä¸ªåˆ†ç‰‡"""
    try:
        async with session.get(m3u8_url, timeout=TIMEOUT_DEEP) as resp:
            if resp.status != 200:
                return None
            text = await resp.text()
            base_url = m3u8_url.rsplit('/', 1)[0] + '/' if '/' in m3u8_url else ''
            shards = []
            for line in text.splitlines():
                line = line.strip()
                if line and not line.startswith('#') and line.endswith('.ts'):
                    shards.append(urljoin(base_url, line))
                    if len(shards) >= TEST_SHARD_COUNT:
                        break
            return shards if shards else None
    except Exception:
        return None

async def test_m3u8_async(session, m3u8_url):
    """å¼‚æ­¥æµ‹è¯•1080P m3u8æµï¼šåˆ†ç‰‡å¤§å°+é€Ÿåº¦åŒé‡éªŒè¯"""
    shards = await parse_m3u8_async(session, m3u8_url)
    if not shards:
        return False
    # å¹¶å‘æµ‹è¯•åˆ†ç‰‡
    tasks = [async_download_hd(session, shard, MIN_HD_SHARD_SIZE) for shard in shards]
    results = await asyncio.gather(*tasks)
    # ç»Ÿè®¡æœ‰æ•ˆç»“æžœï¼ˆåŒæ—¶æ»¡è¶³ï¼šä¸‹è½½æˆåŠŸ+åˆ†ç‰‡â‰¥100KBï¼‰
    cost_times = []
    for t, ok, b in results:
        if ok and b >= MIN_HD_SHARD_SIZE:
            cost_times.append(t)
    if not cost_times:
        return False
    # 1080Pé˜ˆå€¼åˆ¤å®š
    fail_rate = (len(results) - len(cost_times)) / len(results)
    avg_time = statistics.mean(cost_times)
    max_time = max(cost_times)
    return (fail_rate <= FAIL_RATE_THRESHOLD and
            avg_time <= AVG_TIME_THRESHOLD and
            max_time <= MAX_TIME_THRESHOLD)

async def test_flv_async(session, flv_url):
    """å¼‚æ­¥æµ‹è¯•1080P flvæµï¼šä¸‹è½½200KBéªŒè¯å¤§å°+é€Ÿåº¦"""
    cost_time, ok, total_bytes = await async_download_hd(session, flv_url, 204800)
    # 1080P FLVè¦æ±‚ï¼šä¸‹è½½æˆåŠŸ+â‰¥200KB+è€—æ—¶â‰¤é˜ˆå€¼
    return ok and total_bytes >= 204800 and cost_time <= MAX_TIME_THRESHOLD

async def test_url_async(session, name, url, result_queue):
    """å¼‚æ­¥æµ‹è¯•å•ä¸ªåœ°å€ï¼šè½»é‡GETæ£€æµ‹â†’æ·±åº¦æµ‹é€Ÿ"""
    print(f"æµ‹è¯•ä¸­ï¼š{name} | {url[:60]}...", end=" ")
    # ç¬¬ä¸€æ­¥ï¼šè½»é‡GETæ£€æµ‹ï¼ˆæ›¿ä»£HEADï¼Œå…¼å®¹æ‰€æœ‰æœåŠ¡å™¨ï¼‰
    if not await async_light_check(session, url):
        print("âŒ é“¾æŽ¥ä¸å¯è¾¾/æ— æ•ˆ")
        return
    # ç¬¬äºŒæ­¥ï¼šæ·±åº¦æµ‹é€Ÿï¼ˆ1080Pæ ‡å‡†ï¼‰
    if url.endswith('.m3u8'):
        is_smooth = await test_m3u8_async(session, url)
    elif url.endswith('.flv'):
        is_smooth = await test_flv_async(session, url)
    else:
        print("âŒ éžm3u8/flvåè®®")
        return
    # ç»“æžœå…¥é˜Ÿ
    if is_smooth:
        print("âœ… 1080Pæµç•…ï¼ˆä¿ç•™ï¼‰")
        await result_queue.put((name, url))
    else:
        print("âŒ 1080På¡é¡¿/ä½Žæ¸…")

# ===================== ä¸»é€»è¾‘ï¼ˆä¿ç•™åŽŸå§‹æ ¼å¼+æžé€Ÿè¿è¡Œï¼‰=====================
def download_original_txt():
    """åŒæ­¥ä¸‹è½½åŽŸå§‹txtï¼ˆä»…ä¸€æ¬¡ï¼Œè€—æ—¶å¯å¿½ç•¥ï¼‰"""
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}
        resp = requests.get(RAW_TXT_URL, headers=headers, timeout=10)
        resp.raise_for_status()
        resp.encoding = "utf-8"
        return [line.rstrip('\n') for line in resp.text.splitlines() if line.strip()]
    except Exception as e:
        print(f"âŒ ä¸‹è½½åŽŸå§‹æ–‡ä»¶å¤±è´¥ï¼š{e}")
        return []

async def main_async():
    print("="*70)
    print(f"IPTVæžé€Ÿæµ‹é€Ÿï¼ˆ1080Pï¼‰| {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"å¹¶å‘æ•°ï¼š{CONCURRENT_LIMIT} | è½»é‡æ£€æµ‹{TIMEOUT_LIGHT}s / æ·±åº¦æµ‹é€Ÿ{TIMEOUT_DEEP}s")
    print("="*70)
    
    # 1. ä¸‹è½½åŽŸå§‹txtï¼ˆä¿ç•™æ‰€æœ‰æ ¼å¼ï¼‰
    original_lines = download_original_txt()
    if not original_lines:
        print("âŒ æ— åŽŸå§‹æ•°æ®ï¼Œç»ˆæ­¢æµç¨‹")
        return
    
    # 2. è§£æžåŽŸå§‹è¡Œï¼šåˆ†ç¦»åˆ†ç±»è¡Œ/åœ°å€è¡Œ/æ›´æ–°æ—¶é—´è¡Œ
    genre_map = {}        # åˆ†ç±»è¡Œä½ç½®æ˜ å°„ {åœ°å€ç´¢å¼•: åˆ†ç±»è¡Œ}
    url_tasks = []        # å¾…æµ‹è¯•åœ°å€ [(name, url)]
    update_time_line = "" # æ›´æ–°æ—¶é—´è¡Œ
    current_datetime = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    for line in original_lines:
        if line.startswith("ðŸ•˜ï¸") and "#genre#" in line:
            # å¤„ç†æ›´æ–°æ—¶é—´è¡Œï¼Œä¿ç•™åŽŸå§‹æ ¼å¼
            update_time_line = f"ðŸ•˜ï¸{current_datetime},#genre#"
        elif "#genre#" in line and not line.startswith("ðŸ•˜ï¸"):
            # è®°å½•åˆ†ç±»è¡Œï¼Œå…³è”åŽç»­åœ°å€
            genre_map[len(url_tasks)] = line
        elif "," in line:
            # è§£æžåœ°å€è¡Œï¼Œä»…åˆ†å‰²ç¬¬ä¸€ä¸ªé€—å·ï¼ˆå…¼å®¹é“¾æŽ¥å«é€—å·ï¼‰
            name_part, url_part = line.split(",", 1)
            name = name_part.strip()
            url = url_part.strip()
            if url.startswith(SUPPORTED_PROTOCOLS):
                url_tasks.append((name, url))

    # 3. å¼‚æ­¥å¹¶å‘æµ‹è¯•æ‰€æœ‰åœ°å€ï¼ˆæ ¸å¿ƒæé€Ÿï¼‰
    result_queue = asyncio.Queue()
    # åˆ›å»ºå¼‚æ­¥Sessionï¼šå¤ç”¨è¿žæŽ¥+è®¾ç½®UAï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨ï¼Œé˜²å±è”½ï¼‰
    connector = aiohttp.TCPConnector(limit=CONCURRENT_LIMIT)
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}
    async with aiohttp.ClientSession(connector=connector, headers=headers) as session:
        tasks = [test_url_async(session, n, u, result_queue) for n, u in url_tasks]
        await asyncio.gather(*tasks)

    # 4. æ•´ç†ç»“æžœï¼šä¸¥æ ¼ä¿ç•™åŽŸå§‹åˆ†ç±»ç»“æž„å’Œæ ¼å¼
    smooth_urls = []
    while not result_queue.empty():
        smooth_urls.append(await result_queue.get())
    smooth_urls = sorted(smooth_urls, key=lambda x: url_tasks.index((x[0], x[1])))  # ä¿ç•™åŽŸå§‹é¡ºåº

    output_lines = [update_time_line] if update_time_line else []
    current_url_idx = 0
    # æŒ‰åŽŸå§‹é¡ºåºæ’å…¥åˆ†ç±»è¡Œå’Œå¯¹åº”åœ°å€
    sorted_genre = sorted(genre_map.items(), key=lambda x: x[0])
    for idx, genre_line in sorted_genre:
        output_lines.append(genre_line)
        # æ’å…¥è¯¥åˆ†ç±»ä¸‹çš„æµç•…åœ°å€
        while current_url_idx < len(smooth_urls) and current_url_idx < len(url_tasks) and url_tasks.index(smooth_urls[current_url_idx]) >= idx:
            if current_url_idx < len(smooth_urls):
                n, u = smooth_urls[current_url_idx]
                output_lines.append(f"{n},{u}")
                current_url_idx += 1
            else:
                break
    # è¡¥å……å‰©ä½™æ— åˆ†ç±»çš„æµç•…åœ°å€
    while current_url_idx < len(smooth_urls):
        n, u = smooth_urls[current_url_idx]
        output_lines.append(f"{n},{u}")
        current_url_idx += 1

    # 5. å†™å…¥ç»“æžœï¼šä¸¥æ ¼åŒ¹é…åŽŸå§‹txtæ ¼å¼ï¼ˆæ— ä»»ä½•å¤šä½™å­—ç¬¦ï¼‰
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(output_lines))

    # 6. ç»Ÿè®¡è¾“å‡º
    print("="*70)
    print(f"âœ… 1080Pæµ‹é€Ÿå®Œæˆ | æ€»æµ‹è¯•åœ°å€ï¼š{len(url_tasks)} | ä¿ç•™æµç•…åœ°å€ï¼š{len(smooth_urls)}")
    print(f"ðŸ“„ ç»“æžœæ–‡ä»¶ï¼šä»“åº“æ ¹ç›®å½•/{OUTPUT_FILE}ï¼ˆæ ¼å¼ä¸ŽåŽŸå§‹å®Œå…¨ä¸€è‡´ï¼‰")
    print("="*70)

if __name__ == "__main__":
    # é€‚é…Windows/Linuxå¼‚æ­¥è¿è¡Œï¼ˆè§£å†³GitHub Actions/Ubuntuå…¼å®¹é—®é¢˜ï¼‰
    try:
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    except:
        pass
    asyncio.run(main_async())
